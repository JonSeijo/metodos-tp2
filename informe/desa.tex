\section{kNN}

\todo[inline]{Como esta representada nuestra informacion}

\todo[inline]{Que es knn}

\todo[inline]{Por que nos sirve en nuestro problema? como lo usamos?}

\todo[inline]{Usar alguna imagen ilustrativa}

\todo[inline]{Implementacion, quiza hablar sobre optimizaciones y estructuras, minheap y eso}

\todo[inline]{Cuales son los posibles problemas que traeria? Dimensionalidad. Aclarar que corroboraremos esto en la seccion de experimentacion}


\section{PSA}

\todo[inline]{Que es psa}

\todo[inline]{Por que usariamos psa en nuestro problema}

\todo[inline]{Explicar con detalle el metodo}

\todo[inline]{Mencionar que necesitamos los autovec pero explicar como en la seccion siguiente}

\todo[inline]{Posibles problemas? Muy costoso crear la matriz de covarianza si tenemos una muestra muy grande}



\section{Deflación y método de la potencia}

Cómo habíamos mencionado, para poder diagonalizar la matriz de covarianza necesitamos conseguir los autovalores y autovectores. Conseguir los autovalores de la manera tradicional implica encontrar las raíces de un polinomio de grado 784, y no es algo que estemos dispuestos a intentar. Utilizaremos entonces el método iterativo conocido como \textit{Método de la potencia}.
\begin{algorithm}
    \caption{Método de la potencia (Matriz $B$, Vector $x_0$, Int $iters$)}
    \begin{algorithmic}[h]
        \State{$v \gets x_0$}
        \For{$i \gets 1$ to $iters$}
            \State{$v \gets \frac{Bv}{\norm{Bv}}$} \\
        \EndFor
        \State  $\lambda \gets \frac{v^tBv}{v^tv}$
        \State return $(\lambda, v)$

    \end{algorithmic}
\end{algorithm}

Podemos estar seguros de que los autovalores son números reales positivos, pues la matriz de covarianza es una matriz simétrica. Además, también por ser simétrica sabemos que vamos a poder conseguir una base de autovectores ortonormales, que es exactamente lo que necesitamos. \\

Con el método de la potencia conseguimos únicamente el autovalor dominante (y un autovector correspondiente). Como queremos conseguir una base, extendemos el método de la potencia utilizando el método de deflación. Esto funciona por la siguiente observación:

$$ B - \lambda v_1 {v_1}^{t} \text{\hspace{1cm}tiene los mismos autovalores que B (excepto $\lambda$).} $$

Por lo tanto, cuando apliquemos el método de la potencia en la matriz resultante, el autovalor que conseguiremos será distinto del $\lambda$ obtenido inicialmente. \\

Otro detalle es que el vector inicial $x_0$ es elegido aleatoriamente. Si bien es posible en teoría que con un cierto $x_0$ el método no converja, en la práctica eso no sucede. En primer lugar, hay muy pocas probabilidades de tomarlo, y en segundo lugar, los errores de precisión juegan a nuestro favor, ya que con una mínima desviación el $x_0$ deja de ser problemático. \\

\todo[inline]{El último factor a tener en cuenta es la cantidad de iteraciones para la convergencia. Para que pasen los tests de la cátedra es necesario hacer más de 1200 iteraciones. Sin embargo, como veremos en la sección de experimentación, con una cantidad muchísimo menor de iteraciones la accuaracy no se modifica, por lo que vamos a tomar EQUIS iteraciones para optimizar el tiempo de ejecución.\\
 Criterio de convergencia, cant de iteraciones. IDEA: Experimento con un cierto set de datos (pequeño). dejar todo fijo e ir cambiando la cantidad de iteraciones y ver como varia el accuaracy y el tiempo.}





\section{K cross fold validation}

\todo[inline]{Que es?}

\todo[inline]{Por que nos sirve? que buscamos lograr? (evitar overfitting)}

\todo[inline]{Alguna imagen ilustrativa}

\todo[inline]{No se si vale la pena mencionar la implementacion, pensar que mas}


